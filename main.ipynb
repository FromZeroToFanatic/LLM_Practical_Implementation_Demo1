{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459f026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers: HuggingFace的模型与Tokenizer库\n",
    "# datasets: 数据处理与评估（部分功能）\n",
    "# evaluate: BLEU评估工具\n",
    "# sentencepiece: T5使用的分词器依赖\n",
    "# matplotlib: 绘制收敛曲线\n",
    "!pip install transformers datasets evaluate sentencepiece matplotlib -q\n",
    "\n",
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5ForConditionalGeneration, AutoTokenizer, AdamW\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import evaluate\n",
    "\n",
    "# 检查是否有GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"当前使用设备:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cff40b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    \"\"\"\n",
    "    自定义问答数据集类：\n",
    "    - 将 context 和 question 拼接为输入\n",
    "    - 将 answer 作为目标序列\n",
    "    \"\"\"\n",
    "    def __init__(self, file_path, tokenizer, max_input_len=256, max_output_len=64):\n",
    "        self.data = [json.loads(line) for line in open(file_path, 'r', encoding='utf-8')]\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_input_len = max_input_len\n",
    "        self.max_output_len = max_output_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        input_text = f\"context: {item['context']} question: {item['question']}\"\n",
    "        target_text = item['answer']\n",
    "\n",
    "        input_enc = self.tokenizer(\n",
    "            input_text, truncation=True, padding='max_length',\n",
    "            max_length=self.max_input_len, return_tensors='pt'\n",
    "        )\n",
    "        target_enc = self.tokenizer(\n",
    "            target_text, truncation=True, padding='max_length',\n",
    "            max_length=self.max_output_len, return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_enc['input_ids'].squeeze(),\n",
    "            'attention_mask': input_enc['attention_mask'].squeeze(),\n",
    "            'labels': target_enc['input_ids'].squeeze()\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a717076",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"uer/t5-base-chinese-cluecorpussmall\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "train_path = \"F:/ProjectDemo/Demo_1/Data/train.json\"\n",
    "dev_path = \"F:/ProjectDemo/Demo_1/Data/dev.json\"\n",
    "\n",
    "train_dataset = QADataset(train_path, tokenizer, max_input_len=256, max_output_len=64)\n",
    "dev_dataset = QADataset(dev_path, tokenizer, max_input_len=256, max_output_len=64)\n",
    "\n",
    "# 截取少量样本以加速训练\n",
    "#train_dataset.data = train_dataset.data[:100]\n",
    "#dev_dataset.data = dev_dataset.data[:20]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=4)\n",
    "\n",
    "print(\"训练样本数量:\", len(train_dataset))\n",
    "print(\"验证样本数量:\", len(dev_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bf0eccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForConditionalGeneration.from_pretrained(model_name).to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "print(\"模型加载完成，参数量:\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1821b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import autocast, amp\n",
    "scaler = amp.GradScaler('cuda')\n",
    "\n",
    "def train_model_amp(model, train_loader, optimizer, epochs=1, accumulation_steps=1):\n",
    "    train_losses = []\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        epoch_loss = 0.0\n",
    "        loop = tqdm(train_loader, desc=f\"Epoch {epoch + 1}/{epochs}\", ncols=100)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        for step, batch in enumerate(loop):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "\n",
    "            # AMP 混合精度训练\n",
    "            with autocast(device_type='cuda'):\n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss / accumulation_steps\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "\n",
    "            if (step + 1) % accumulation_steps == 0:\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            epoch_loss += loss.item() * accumulation_steps\n",
    "            loop.set_postfix(loss=loss.item() * accumulation_steps)\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        train_losses.append(avg_loss)\n",
    "        print(f\"Epoch {epoch + 1} Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return train_losses\n",
    "\n",
    "EPOCHS = 5\n",
    "ACCUMULATION_STEPS = 2\n",
    "train_losses = train_model_amp(model, train_loader, optimizer,\n",
    "                               epochs=EPOCHS,\n",
    "                               accumulation_steps=ACCUMULATION_STEPS)\n",
    "\n",
    "plt.plot(train_losses, marker='o', label=\"Training Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Training Loss Curve (AMP)\")\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33229eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "bleu_metric = evaluate.load(\"sacrebleu\")\n",
    "\n",
    "def evaluate_bleu(model, data_loader, tokenizer):\n",
    "    model.eval()\n",
    "    preds, refs = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(data_loader, desc=\"Evaluating\", ncols=100):\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels']\n",
    "\n",
    "            outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_length=64)\n",
    "            pred_texts = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "            label_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "\n",
    "            preds.extend(pred_texts)\n",
    "            refs.extend([[text] for text in label_texts])\n",
    "\n",
    "    bleu_score = bleu_metric.compute(predictions=preds, references=refs)\n",
    "    print(\"BLEU分数:\", bleu_score)\n",
    "\n",
    "    for i in range(3):\n",
    "        print(f\"预测: {preds[i]} | 真实: {refs[i][0]}\")\n",
    "\n",
    "    return bleu_score\n",
    "\n",
    "evaluate_bleu(model, dev_loader, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a413a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, tokenizer, context, question):\n",
    "    model.eval()\n",
    "    input_text = f\"context: {context} question: {question}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt', truncation=True, max_length=256).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(input_ids=input_ids, max_length=64, num_beams=4)\n",
    "\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# 测试预测\n",
    "test_context = \"违规分为:一般违规扣分、严重违规扣分、出售假冒商品违规扣分,淘宝网每年12月31日24:00点会对符合条件的扣分做清零处理。\"\n",
    "test_question = \"淘宝扣分什么时候清零\"\n",
    "print(\"预测答案:\", predict(model, tokenizer, test_context, test_question))"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
